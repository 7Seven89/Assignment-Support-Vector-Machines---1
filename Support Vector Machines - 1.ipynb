{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c28e0c-c7bc-45c9-8176-6bcd57840c17",
   "metadata": {},
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear SVM is given by:\n",
    "\n",
    "For a linear SVM, the decision boundary is represented by:\n",
    "\n",
    "\\[\n",
    "w \\cdot x + b = 0\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( w \\) represents the weight vector,\n",
    "- \\( x \\) is an input data point (feature vector),\n",
    "- \\( b \\) is the bias term.\n",
    "\n",
    "### Objective:\n",
    "The goal is to find the hyperplane (decision boundary) that separates the two classes in the feature space while maximizing the margin between them.\n",
    "\n",
    "# Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear SVM is to:\n",
    "\\[\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\max(0, 1 - y_i (w \\cdot x_i + b))\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( \\frac{1}{2} \\|w\\|^2 \\) is the regularization term that penalizes large values of \\( w \\), encouraging sparsity.\n",
    "- \\( C \\) is a regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "- \\( \\max(0, 1 - y_i (w \\cdot x_i + b)) \\) is the hinge loss, which measures the classification error.\n",
    "\n",
    "# Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick in SVM is a mathematical approach to transform the data into a higher-dimensional space without explicitly computing the mapping function. This allows SVM to perform non-linear classification by only working in the original input space using kernel functions like:\n",
    "- Linear Kernel: \\( K(x, y) = x \\cdot y \\)\n",
    "- Polynomial Kernel: \\( K(x, y) = (x \\cdot y + 1)^d \\)\n",
    "- Radial Basis Function (RBF) Kernel: \\( K(x, y) = e^{-\\gamma \\|x - y\\|^2} \\)\n",
    "\n",
    "The kernel trick avoids the need to explicitly compute \\( x_i \\cdot x_j \\), making SVMs more scalable to higher-dimensional feature spaces.\n",
    "\n",
    "# Q4. What is the role of support vectors in SVM? Explain with example.\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane) in an SVM. These points have the maximum influence on the position and orientation of the decision boundary.\n",
    "\n",
    "Example:\n",
    "Consider a simple binary classification scenario with two classes (e.g., \"+\" and \"−\"):\n",
    "1. Points closer to the decision boundary help determine where the hyperplane is positioned.\n",
    "2. Support vectors, which are the data points closest to the hyperplane, directly influence its location.\n",
    "3. They play a crucial role in maximizing the margin between the two classes.\n",
    "\n",
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "\n",
    "**Hyperplane**:\n",
    "- A hyperplane is a flat decision boundary that divides the space into two regions.\n",
    "- In SVM, this is the line or plane where \\( w \\cdot x + b = 0 \\).\n",
    "\n",
    "**Marginal Plane**:\n",
    "- The two parallel hyperplanes that are equidistant from the decision boundary are called the \"marginal planes\".\n",
    "- These planes separate the support vectors on either side of the decision boundary.\n",
    "- They define the margin, which is the distance between them.\n",
    "\n",
    "**Soft Margin**:\n",
    "- In the case of a soft margin, misclassifications are allowed but penalized.\n",
    "- The decision boundary is less strict and includes some \"slack\" in terms of errors.\n",
    "- This is controlled by the regularization parameter \\( C \\).\n",
    "- Example: Data points classified correctly but close to the decision boundary are allowed to be slightly misclassified.\n",
    "\n",
    "**Hard Margin**:\n",
    "- The hard margin solution is a special case where no errors are allowed.\n",
    "- Only correctly classified data points are considered for positioning the hyperplane.\n",
    "- The decision boundary is strictly linear and does not allow any errors.\n",
    "- In practice, hard margins can lead to overfitting on noisy datasets.\n",
    "\n",
    "# Q6. SVM Implementation through Iris Dataset\n",
    "\n",
    "**Steps to Perform**:\n",
    "1. **Load the Iris Dataset** from the scikit-learn library and split it into a training set and a testing set.\n",
    "2. **Train a linear SVM classifier** on the training set and predict the labels for the testing set.\n",
    "3. **Compute the accuracy of the model** on the testing set.\n",
    "4. **Plot the decision boundaries** of the trained model using two of the features.\n",
    "5. **Try different values of the regularisation parameter \\( C \\)** and see how it affects the performance of the model.\n",
    "\n",
    "### Bonus Task:\n",
    "**Implement a Linear SVM Classifier from Scratch** using Python and compare its performance with the scikit-learn implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ff3fd-4630-4983-b575-f608d7b3230d",
   "metadata": {},
   "source": [
    "## Q1. What is the mathematical formula for a linear SVM?\n",
    "The mathematical formula for a linear SVM classifier is:\n",
    "\n",
    "\\[\n",
    "f(x) = \\text{sign}(w^T x + b)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( w \\) is the weight vector,\n",
    "- \\( x \\) is the input feature vector,\n",
    "- \\( b \\) is the bias term.\n",
    "\n",
    "The decision boundary is defined as:\n",
    "\n",
    "\\[\n",
    "w^T x + b = 0\n",
    "\\]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc153c9a-ccf0-4922-8730-809ef4bcdd3e",
   "metadata": {},
   "source": [
    "## Q2. What is the objective function of a linear SVM?\n",
    "The objective of a linear SVM is to maximize the margin between the decision boundary and the closest data points (support vectors) while minimizing classification errors. This is achieved by solving:\n",
    "\n",
    "\\[\n",
    "\\text{Minimize: } \\frac{1}{2} ||w||^2\n",
    "\\]\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "\\[\n",
    "y_i (w^T x_i + b) \\geq 1, \\forall i\n",
    "\\]\n",
    "\n",
    "For soft-margin SVM, a penalty is added for misclassified points:\n",
    "\n",
    "\\[\n",
    "\\text{Minimize: } \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n \\xi_i\n",
    "\\]\n",
    "\n",
    "Where \\( \\xi_i \\) are slack variables, and \\( C \\) is the regularization parameter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055beeee-854f-4a6d-ba9f-46ce451b7b68",
   "metadata": {},
   "source": [
    "## Q3. What is the kernel trick in SVM?\n",
    "The kernel trick allows SVM to classify data that is not linearly separable by implicitly mapping the input features into a higher-dimensional space where a linear separation is possible. Common kernels include:\n",
    "- **Linear Kernel**: \\( K(x, x') = x^T x' \\)\n",
    "- **Polynomial Kernel**: \\( K(x, x') = (x^T x' + c)^d \\)\n",
    "- **Radial Basis Function (RBF) Kernel**: \\( K(x, x') = \\exp(-\\gamma ||x - x'||^2) \\)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c2664-9b5e-4859-90ee-5f0b98e1569f",
   "metadata": {},
   "source": [
    "## Q4. What is the role of support vectors in SVM? \n",
    "Support vectors are the data points closest to the decision boundary (hyperplane). They determine the position and orientation of the hyperplane and influence the SVM model's performance.\n",
    "\n",
    "**Example:**\n",
    "Consider a binary classification problem with two classes (blue and red). The support vectors are the points from each class closest to the decision boundary. Removing non-support vector points does not affect the hyperplane.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be91f99-4854-48d3-9d10-0f8afe0c852b",
   "metadata": {},
   "source": [
    "## Q5. Illustrate with Examples and Graphs:\n",
    "1. **Hyperplane**: The decision boundary separating the classes.\n",
    "2. **Marginal Plane**: Parallel lines to the hyperplane marking the margin boundary.\n",
    "3. **Soft Margin**: Allows some misclassification with a penalty.\n",
    "4. **Hard Margin**: Assumes perfect separability with no misclassification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4f8dc-d531-4e15-9e41-9b7198735fd5",
   "metadata": {},
   "source": [
    "## Q6. SVM Implementation through Iris Dataset\n",
    "\n",
    "### Steps:\n",
    "1. **Load the Dataset**:\n",
    "   Use scikit-learn's `load_iris` function to load the Iris dataset and split it into training and testing sets.\n",
    "\n",
    "2. **Train the Model**:\n",
    "   Use `SVC` from scikit-learn with a linear kernel to train the SVM model.\n",
    "\n",
    "3. **Evaluate the Model**:\n",
    "   Compute accuracy on the test set.\n",
    "\n",
    "4. **Plot Decision Boundaries**:\n",
    "   Visualize decision boundaries for two features of the dataset.\n",
    "\n",
    "5. **Experiment with C**:\n",
    "   Train the model with different \\( C \\) values and observe performance changes.\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification (for simplicity)\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "model = SVC(kernel='linear', C=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(X, y, model):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('SVM Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca17898-d28e-4573-b811-a46962dd5cdb",
   "metadata": {},
   "source": [
    "## Bonus Task: Implementing a Linear SVM from Scratch\n",
    "\n",
    "### Overview:\n",
    "Implementing a linear Support Vector Machine (SVM) from scratch involves:\n",
    "\n",
    "1. **Calculating the Optimal Weights (𝑤) and Bias (𝑏)**:\n",
    "   - The objective is to find the optimal hyperplane that maximizes the margin between two classes while minimizing classification errors.\n",
    "   - The optimal weights and bias can be calculated using methods like **gradient descent** or **quadratic programming**.\n",
    "\n",
    "2. **Predicting Labels Using the Decision Boundary**:\n",
    "   - Once the optimal weights and bias are found, we use the decision function:\n",
    "     \\[\n",
    "     f(x) = w \\cdot x + b\n",
    "     \\]\n",
    "     - If \\( f(x) \\geq 0 \\), predict class 1; otherwise, predict class 0.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "#### 1. **Calculate Optimal Weights and Bias**:\n",
    "   - The model aims to minimize the hinge loss function:\n",
    "     \\[\n",
    "     L(w, b) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\max(0, 1 - y_i(w \\cdot x_i + b))\n",
    "     \\]\n",
    "     - Here, \\( C \\) is a regularization parameter that controls the trade-off between maximizing the margin and minimizing misclassification.\n",
    "\n",
    "   - **Gradient Descent** is used to iteratively update the weights \\( w \\) and bias \\( b \\):\n",
    "     - Compute the gradient of the loss with respect to \\( w \\) and \\( b \\).\n",
    "     - Update the weights and bias using the learning rate.\n",
    "\n",
    "#### 2. **Predict Labels**:\n",
    "   - Once the model is trained, we predict the labels by evaluating the decision function:\n",
    "     \\[\n",
    "     \\hat{y} = \\text{sign}(w \\cdot x + b)\n",
    "     \\]\n",
    "     - The predicted label \\( \\hat{y} \\) is either class 0 or class 1 based on the sign of the decision function.\n",
    "\n",
    "#### 3. **Comparison with Scikit-learn**:\n",
    "   - The custom implementation from scratch is compared to the scikit-learn SVM (`SVC`) implementation.\n",
    "   - By evaluating both models on the same dataset (e.g., Iris dataset), we can compare performance metrics such as accuracy and the computational complexity of each method.\n",
    "\n",
    "### Conclusion:\n",
    "- Implementing a linear SVM from scratch provides insights into optimization nuances and helps in understanding how hyperparameters (like \\( C \\)) and the choice of optimization methods (gradient descent vs quadratic programming) affect model performance.\n",
    "- This task also highlights the trade-offs between custom implementations and highly optimized library functions like those in scikit-learn.\n",
    "\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for simplicity\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification (classes 0 and 1)\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# SVM from scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y == 0, -1, 1)  # Map labels {0, 1} to {-1, 1}\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.w) + self.b\n",
    "        return np.where(linear_output >= 0, 1, 0)\n",
    "\n",
    "# Train custom SVM\n",
    "custom_svm = LinearSVM()\n",
    "custom_svm.fit(X_train, y_train)\n",
    "y_pred_custom = custom_svm.predict(X_test)\n",
    "\n",
    "# Accuracy of custom SVM\n",
    "custom_accuracy = accuracy_score(y_test, y_pred_custom)\n",
    "print(f\"Custom SVM Accuracy: {custom_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compare with scikit-learn SVM\n",
    "sklearn_svm = SVC(kernel='linear', C=1.0)\n",
    "sklearn_svm.fit(X_train, y_train)\n",
    "y_pred_sklearn = sklearn_svm.predict(X_test)\n",
    "\n",
    "# Accuracy of scikit-learn SVM\n",
    "sklearn_accuracy = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Scikit-learn SVM Accuracy: {sklearn_accuracy * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
